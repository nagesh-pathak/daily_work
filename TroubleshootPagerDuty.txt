Transcript
September 25, 2025, 6:34AM
Vaibhav Shetty started transcription
Narasimha Reddy   0:04This is the CDC dot floor dot API dot service.OK.I hope everyone already knows that one. If not, please let me know. ADC flow API is there. OK under which repo CDC flow dashboard for there. All the dashboards are added here. It is in master.Currently the master is not up to up to date. Let's go with this.Latest version.OK.These are all categorized into two things. One is that the alerts and another one is that monitoring. Let's go with this alerts alerts dot basin. Are they right?Total 14 alerts are there, remaining all considered as a.Dashboard OK merging one for that how we can check that one. SAS ecosystem is a folder name. If you go go to that SAS ecosystem that is a previous one.New one is a split into that in the two parts, SAS ecosystem alerting, SAS ecosystem monitoring. Let's open this too.So these two changes are are added.Who is that? Yeah, yeah.
Raja Nand Sharma   1:41Raja.Nasma, how is this pushed off to Grafana? Like how is this Jason? Like how does Grafana know that it has to render these JSONS?
Narasimha Reddy   1:52OK, we're coming to that. OK, these are the the changes, right? OK, I hope the one is that repo and then the folder right folder is ecosystem.
Raja Nand Sharma   1:54OK.
Narasimha Reddy   2:11One second.It has.Second thing is that.Repo is over and we have the data OK and how the deployment happens, right?I also know less only Nagesh was involved into that one, but we will try to share you the details there.Where are you, honey?OK.D minus and CDC flow get Grafana dashboards.In CDC flow namespace under this component that kind Grafana dashboards we can see all this. So we need to check that how it will be communicating with the Grafana correct? So now we can go and check this.The templating part templates. We have a dashboard, dashboard widget, dashboard view. These are all different. Let's go to the dashboard Grafana. So kind is a dashboard. OK, it is a similar to that one Raja CDC.CDs and CDC CRD survey, right? So Kubernetes controller is there. So these all.
Raja Nand Sharma   4:03Thank you.OK.OK, OK. So like Grafana will look for all the Grafana dashboard kind resources and then deploy them. OK, OK, OK, OK.
Narasimha Reddy   4:13Correct, correct, correct. Yeah. So there there it will be getting all these updates and then it will be syncing up that one. So this is that the kind we are in the currently using Grafana dashboard.
Raja Nand Sharma   4:20Mm-hmm.
Narasimha Reddy   4:29The CRD changes any home that isn't controlled by platform level we are using as a CS.OK.Deployment term seed.CRD to CR that is a graph ****.So if anyone wants to add in a new one, you can create with this kind and I can take that complete one. What are the required things? Jason specifically you can add that Jason content and data sources is a Prometheus. Sometimes it may be a different right?So that it will pick the related source. We have seen Redshift, the source and Postgres different sources.Deployment side is also clear. Only thing is that we need to update the respective the SAS ecosystem folder, but in the current one it is under the CDC flow API service repo dashboards in the in the folder.Once you update and deploy it then we can it will be reflected here from monitoring and alerting. So alerts can be verified all the alerts once it is the active.That means added to the dashboards will be present here. We have segregated that into three parts. One is that CDC is the application name so that we can get all the CDC related. Another one is a segregated into.C to C cloud to cloud CDCC to C OK, these are the cloud to cloud alerts. One more is the CDC cloud. That means CDC flow API, right?So these are all CDC flow and API related. If if you need only flow specific, this is the CDC flow specific. If you want only API specific you can get the API specific and if you want to go to that onprem specific alerts.Can check it and these are the onprems. So it is easy for us to go through these highlights and take some action. Yes Raja.
Raja Nand Sharma   7:03So Nasima, what about data exporter Kafka and Kafka flow processor pods?
Narasimha Reddy   7:07Those are not added even marketplace controller and CDCCRDS. OK, we need to add that one. Data exporter Kafka is the one issue with that one is the we are unable to fetch some of the data because app label is not added.
Raja Nand Sharma   7:11Mm-hmm.Mhm.OK, mm-hmm.App label is not added for detect photographer.
Narasimha Reddy   7:27OK.Yes, one second. I will show that one.
Raja Nand Sharma   7:32Oh.Oh, OK.
Narasimha Reddy   7:37This is the CDC flow. OK, I need to get this. Anyhow, you rise that can question. Let me clarify that to you.And maybe small?
Raja Nand Sharma   7:58OK, you're saying labels, right? Data exporter Kafka has, yeah.
Narasimha Reddy   7:59This so these are the app labels right? So the data exporter Kafka the labels are added like this is slash then managed device something OK and when we are trying with that one labels it was failing OK we need to find a way.
Raja Nand Sharma   8:03Yeah.Yes, yeah, yeah, yeah, yeah.Mhm.
Narasimha Reddy   8:16How we can fetch that one?
Raja Nand Sharma   8:18Oh.
Narasimha Reddy   8:20OK, that is the one thing.
Raja Nand Sharma   8:22OK.OK. And uh, what about?
Narasimha Reddy   8:24ADC flow is there, right?
Raja Nand Sharma   8:27Yeah, yeah. And what about CDC KFP like the KFP and KFM parts?
Narasimha Reddy   8:33That is not in our means in our bucket. So I think that's team is handling that one. There in the we need some additional dashboards and monitoring also for the Kafka lag and other things, right?
Raja Nand Sharma   8:35Mm-hmm. OK. OK.OK.Yeah, yeah, because if some data is not working, it might be from that end as well.
Narasimha Reddy   8:49Oh.Yes, you are right.
Raja Nand Sharma   8:57Thank you.
Narasimha Reddy   8:58Yeah, so these are the total 4 categories. If anyone deployed and have observed some issues like going to trigger that one alert.You can come here and simply resume that one so that we have a control over that one.Oh.And this is that alert rules.One is that the CDC.C2C.Load.No.OK.OK, both sections not pass.OK, I hope this part is also clear. This one anyhow we can also go through that one and this is required.These are that alerts. OK, here the symbol the is a OK one. The coloring is OK. If it is not OK then the color will be kind of heartbreak and symbol that means.Now let is a triggered.This is a Gray color. That means alert is paused. So we need to check that alert the query and alert also. Sometimes select queries might be incorrect and not in a optimized approach.So the load time will be taking more time. So you can review that once and verify it and data source is that important thing here from which data source we are fetching. Prometheus is the one from federated. Prometheus is the another one.For all cloud components we are using that in the for all onprem we are using federated from this and Cortex also. OK, so this needs to be added based on that and the other applications also using onprem OK.Now let's go to the alerting part. Let's explore some of the options, the rule, conditions and what is the message you want. Here it is currently sending this notification to Atlas, onprem and Cloudinfra.And SaaS ecosystem notifications and NEAPRD one. These are the multiple channels you can create and send the notifications. What are the channels are added to that one so the notification will be sending to that one.So SaaS ecosystem notification is the one we will be checking now.So notification channels.As ecosystem notifications. So if you click this one it will give the details of this the notification name and it is on web book type. The URL is that given here. So second thing is that.From where we are collecting this URL? OK, this is a back one second.If you go to octal and then the.Teams is there.Teams wrapper. Click on that one.We need to raise a request for teams wrapper. It's no request.For any new one we need to select that Grapana 9.4.1 and then your team name SAS ecosystem.SYST OK and channel name we need to create CDC.Monitoring whatever is that other one that we can create that in description. Then if you start validation it will validate. Example I'm taking that PRD one not PRD one testing we created right?CDC.Monitoring testing.One second.Created one Grafana testing. OK, let's take this one.EDC monitoring.Testing.OBC validate.OK, validated successfully send the event.We received that the test notification for teams wrapper. So this is how we will be validating and once we submit it then we can get that URL web URL. That URL can be added to this.Notification channel.OK, this is also clear.Let's move to this one. So the same notification can be sent to multiple the channels, also team channels for different channels. Based on that linking we have given here, some of them are the.Pager duty alerts deleted. Some of them are internal path. We'll be checking this one, right?This part is also clear and any queries from your side.So far.OK, when is that done create?Notification.OK, that's all for then.Now if you're all clear on this one, next thing is that OK, we already created a runbooks. OK, we need to check that.As equals system runbooks, Grafana alerts runbook OK.We have onprem specific few alerts and flow specific and cloud to cloud alerts. Let's go through this on onprem first. Anyhow the cloud is that important based thing. Currently it is enabled for the page duty also. Let's check the.Cloud one passed TDC flow.This is the 4/1.OK, CDC flow alerts for cloud. There's a flow request unable to fulfill alert.So we can go to.All right.Electron.Say to uh, CDC.No.This is the one dependent services failure alert. It's a method specific.It is ignore all it.So request unable to fulfill. There are some some number of the requests are there some methods are there right? One is that the get config flow and create the flow data.Different the REST API methods also there right GRPC methods. So if anyone is the failing then we'll get that alerts. So for that we can go and check this what are different types of methods that are there so.And then then the response GRPC codes are there, right? We'll be validating that one. Let's check one of that one A.Get lost.So this is the the code GRPC code. OK, OK, sometimes the code might be different, unauthenticated, permission denied, invalid. OK, so we will be segregating that and based on that that alert will be triggered OK.We have to see that when these rules you can check.I'm just giving high level information. You can you guys can go through that in in detail, OK.I hope this is clear for everyone. If there is any other any errors, OK, the GRPC code might be different other than OK another one is that the 500 error. This is a service down internal error right?If there is any issue happened OK then we can come and check this CDC flow is the under CDC flow namespace.You can check the logs in CDC flow namespace. OK, CDC flow namespace. What are the components are there? You can check that.So we have CDC, API, CDC flow and then status reporter. One more is host, host app sync. These are the four things and then the CDC flow namespace, but CDC flow is just covering this one CDC flow.And host app sync is in the separate in the pod. We need to check that one also. Currently it is not handled in this case. Here we don't have any GRPC methods in post app sync.That is the just the syncing of the post up and fetch the data.And CDC flow it is a supporting the back end for the CSP so it has the some methods OK that can be.Used for responding to the request and pod restarts. So we need to check all these pods then if there is any restart happened or not for that connecting to the perspective cluster PRD one reu come one and check that why the pod is restarted.The pod restart restarted. There may be other reasons right? One is that we need to check that the previous pod error logs and describe the pod. These are the two things which we need to do. Basically we can also verify that other things.OK, if pod restart happened right, what would be the issue if there is some panic?Panic error. So then it will be kill that process right? Or if it isn't out of memory so then that time also the process is will be killed. OK, these are some use cases you can categorize that and then.What are different types of what it starts? Those reasons you can list out and analyze that debug OK.And that is clear pod restart. Same steps required based on that namespace.And 400. Let's check that 400.What are different 400 errors coming and what is that condition added to this one? Currently it isn't added in the 0, so it's more frequently hitting. You can check that in the the respective logs. So either you can check it in the cluster level logs or you can.Go to the Grafana and check the logs. So for that one.Let me.Lock key locks. One is that cloud and another one is that onprem.Loud one.Low key infra logging is the firm.It's all lower the case.Log in so you can.Select this loci log loci infra logging and then select that cluster. By default it's the PRD one only, but as option is there you can select that one.Namespace. Select the namespace CDC flow.So if you select the CDC flow then you can get all the logs. So at that interval time if there is any issue happened, you can see some spike in that error log if you want to go dig deeper into that one.Go with this container ID. Is it a CDC, API, API, status reporter and a host app sync is also there. So here you can filter out the the error log time. Currently it supports the even info level info logs also.Cloud flows, not the onprem flows. You can select the error or you can go with this. The it is based on that not error level is there right?So it is easy for you to if you just select the error, it may be part of message. Also you go with this one, it will be selected only level errors OK.That you know logging part.And onprem, let's check this one.So onprem logs low key new is this one.And then locks.So here cluster PRD one. This is the same one. It will not work here. PRD one is fine now.You can select that the.Container in the ID, agent ID, account ID for onprem logs. There are some additional things also added pool ID, service ID. The pool ID, service ID added I think few years back as part of the the pool concepts for all onprem hosts.It was a supporting lower in grants pooling. Later it isn't moved OK but now what we can go and write we can select that container ID and give that container CDC under score.Yeah, you see under score in.Let me reload the page, OK?This is I'm can lost for me.Continue.Contenter ID.So all onprem logs starts with CDC under score. OK, that is the one easiest way you can select.And then error and equal to log. You don't need to select why? Because all onprem logs are supporting only errors and warnings, right? It is not supporting. It is not enabled with the in for the debug logs, so it is OK to run it.And see that errors that particular part. So as we are discussing only cloud flows now, let's continue that one, OK.The log logs also clarified. Let's move on to this one. A close request unable to fulfill alerts. So we will be checking that the the JRPC code is OK other than that OK.So these are the things you can check it in the in your in the rule rule condition or the alert. OK how important it is. Based on that you can set the condition and what does it mean in service down or something right? So that meaning you can get.What are the the immediate and the actions required? We can update that one accordingly and the steps we have seen right whenever this alert is the record is the flow unable to fulfill. You can go and check this the lock key logs.That is a step one. For that either we can go to the the GR method cluster pod logs or Grafana low key logs. 2 two steps we can check if it isn't done immediately you.Pick that the Grafana on on call. Then you can check in the cluster level. If you attempted that after the one hour or two hours later you want to investigate, you can go to that the Grafana part.If logs are rotated faster, right? That is clear. Any doubt on this one before moving to the next one?OK, let me move to that second one. So close service the down. We already explained that. So the CVIT is a critical and priority is high. Why the flow service down is important because if it is down.Then the that the respective services the unable to connect to the the CDC flow. What could be the issues?What are the services for connecting to CDC flow? We have to identify that what services will get impacted with this one and whether it happened only once or continuously happening. If it's continuously happening, first thing is that we need we need to check this the logs.Pod restart on the any the error logs, right? We can check it in the Grafana or in the pod logs and describe the pod. So you can check this when when you describe the pod, you will check events.It's not described here.It's very big, right?OK, I will describe it now.So when you describe the pod, you will get the details. See this is the context to deadline exceeded. We are getting this one. It's unhealthy currently.So the events that will give the details the step by step the 6 minutes.Ago the age OK this is that the unhealthy status can also check that the reasons OK unhealthy. What are the steps are there right and status is another one type status. Multiple statuses are there status.Colon so it is a running status. If it is a failure status then it showed differently. So another one is example. I'm telling this is a different service. You can see this unable to get get data from tied endpoint status.Internal server error so it is showing the error. OK error making call to that red service. When CDC service is the down with 500 error you can check that the pod logs describe the pod.And then check the connection once it is successful or not. So what are the next steps? If we get if it's already issue, we can restart the pod if it may solve the problem sometimes, right?Or the we need to analyze the logs further why it is failing. So due to that the dependencies there may be multiple issues. Example the token got expired, certificate got expired.And any any particular key is not accessible. Different reasons are right. First The thing is that what are the dependent services with this CDC flow? What are the dependent things? What are the required things for the CDC flow?You need to check that the outgoing and the required things for CDC parameter wise you can check it there. Is it a DB related? Any parameters missed or any the expiry related things are there. So that is how.You can start the analysis.Before moving to the next, any queries?OK pod restart. It is a straightforward alert. You can go and check this previous pod error logs.That will give you the details and flow pod restart. There are multiple things. One is the CDC flow and CDC API pod restart or CDC cloud to cloud. Also cloud to cloud. It is A1 customer specific the pod will get impacted if it is CDC flow.Oh.Multiple customers will get impacted, right? You need to understand that the this the CVID also whether it is impacting 1 customer or multiple customers. Based on that we need to take that actions. Sometimes rollout restart also will help, but you need to.Check in which scenario we can use that in the command.400 X We already discussed this one. The steps are same. What are the we are using for 500? We'll use that in the same steps for the four **.Cloud to cloud pod restart discussed. OK.OK, I'm I need to check that window.DDC data flow.There are N number of the pods. OK, this is not like a normal deployment. It's a stateful set. OK, so you you have to understand that the Kubernetes components, what is the stateful set? What is the deployment? So how that in the deployment rollout?Roll out three starts happens so for any particular one if you take the GRPC one, GRPC in and that destination specific pod will be created for one particular flow so.It will be created based on the account ID and the flow ID. If you check one account ID for this one, this is a GRP scene and a HTTP out. So second thing is that we have one config map for each flow.We can check that also here.For any configurations you can check it in the.Understand that how to check this?Describe.When you describe CM, multiple details are there. I mean that metrics, end point, port, other things apart from the.Uh, normal configurations. There are multiple things. OK, this is a flow related JSON configuration.With complete one.It also maintains that the metrics based on the scraping thing.You can check that one also if it is the not working with the metric, we can analyze these the metric counters.Pod failed. Sometimes these cloud to cloud pods will give some issues.Which may cause the CDC flow. Also we need to be more careful when we are analyzing this one. OK, any action you you want to take, you need another person to assist. OK, whether it may be SSOPS or the.You can check with me or someone else, OK?I CPU you say select.Oh, here.Cloud to cloud.This is a cloud to cloud one.Why we missed this the CPU and the memory related here and the nagesh?
Nagesh Pathak   37:54Sorry, Rina Simha.
Narasimha Reddy   37:57We should have when the CPU and the memory related alerts right from the CDC AP and the flow.
Nagesh Pathak   38:12Maybe, yeah, for the next update we can check on this.
Narasimha Reddy   38:17It was there earlier.
Nagesh Pathak   38:20No, though, that's not there.
Narasimha Reddy   38:24Mhm.
Nagesh Pathak   38:26Only monitoring was there for this API inflow.
Narasimha Reddy   38:34OK.Yeah, cloud to cloud flows, then pod restart events we already discussed pod failed. So when you go to this query it will give the details what could be the pod failure alerts in that site. We can check on the condition also.And analyze the logs CPU and the memory alerts. We can set some threshold if it in the.The above this threshold 6060 is the percentage rate average above this one. Then it figures this alerts above 61. It's this is the the right side one. Currently it is alerting is working.It is also similar to that one and network in yeah CPU and memory alerts. How we'll be checking right? We can check that the log and usage of HPA is there right? You can check that one.How much percent is currently is he using? I think we don't have that HPF and CDC enable that.We haven't added that one.Can we get the the top parts?This this is giving the details of the resources only, but not giving the details of what is the current uses.And networking activity. There is no this is related to that the traffic. We can check this one.I hope we covered the required things for the cloud components.Before moving to the onprem components, any queries?No worries.OK, let me quickly finish the tantrum one also.When we are analyzing that on Prem, what are the conditions? One is that.On Prem connect teleport access.I'm going step by step. You need a teleport access. Once you have the teleport access, then for the teleport access then.Connect with OPHID.So what are the the environment? The customer in the environment has issues right? You can get this low PHID and go to that.Some of the servers are listed here with the OPHID's and it also maintains the account ID account under score ID. If you don't know that account ID you can search it here. It's easy but the account name will not be shown here.Example if you take any of the OPHID and then the click on this route right then it will be connecting to that in the host. Better I can give you a quick walk through that one. I would not be taking.PJ Bank Hospital in Hebb, Janson, Kangra.SPT is there.
Vaibhav Shetty   43:04Is it on UCOM or PRD?
Narasimha Reddy   43:06No, no, no. One second. One second. I got it.And going to town.Reporting you OK.Let me continue this one connect to connect with the whole page ID. Once you connect in the with the CH in the route and then the we will start analyzing that the.With a couple of base things, OK, what we'll do that right? Do you pipe and SH? What is that current usage of the disk size and even memory also will check what is the CPU and the memory usage.Then you'll check that what are the flows, what is the state? Is it up and running and on print pods are there right? So we can check that the whether the configurations configurations we cannot get it in the from the local logs right? So for that do we need a?Teleport access so we can log into that the pod from outside the pod. Also we can just run that command CAD command. Then you will get that the configurations. Sometimes it it may be issue with the configurations backward compatibility related issues and all.Let me go through these the alerts and check one by one event received and it delivered a timestamp. So this is the one related to that received and it delivered the data OK.All source data OK will be collected in the source content, GRPC in, RDNS in. So when IP meta RPZ in OK, so we'll take that as a received timestamp.All destination parts when they successfully send that data will take that as a delivered timestamp. So we need to check that some issues happening when received is coming but delivered is.Not updated. Received is the any, but delivery is showing. There may be a case that we have some pending data and we start processing it, but it stopped up.So we stop fetching the data from source after some time. So these are some of the mismatches. If anyone received or delivered is any, we need to start analyzing the data for this.We can check that the each pod logs from where it is failing. These are the metrics that are related, right? Whether that particular pod received the event or not, we can check that one.These steps are the different ones where you will update them.So even if we are collecting the event OK but it is not reflected in the CSP then we need to check that the status flow might be issue. We have to analyze status in the reporter logs if it is failing or not.And then we need to check the CDC flow logs also if there are any errors when we are done updating that the flow status OK for any one item which we are fetching from one source.And then the render in some place, right? What are the components are there involving that in the path? We need to analyze that well, but first analyze that the base part and then go to that till end where it is failing.Any doubt in this one event received and delivered?OK, even received and delivered. This is a just a timestamp value. It is handled in a two ways. One is that legacy model and the new one. New one is based on that the.As explained here now right the Prometheus metrics also you can check like this is in this format config maps. It is storing the data in config maps scraping model and in existing one it is a file based. We will store that into the files.And that is for the persistency.We'll read the data timestamp value and send to the from this federated from this for the onprem related.So you can first step you can go to the federated from this and whatever the timestamp metric term, give that metric name and check that one. So you can also check with the labels. There are multiple labels, account ID, OP ID, service ID or you can add some custom labels also so.Using that one, you can filter out and verify that timestamp is updating in rated Prometheus or not. Rated Prometheus has that one, but it is not reflected in the CDC flow. What are the other components you can?And received and delivered timestamp lag issue. OK, received is updated but delivered is not updated. That is a lag so that we need to analyze that one.These two are covered and this is the onprem host status.The host is also maintain the status whether our host is healthy or not. So if it is unhealthy then the customers may set some alerts the the service is unhealthy, trigger the mail or something. So for that one we can go and check that the our our monitoring.OK, what we need to do is just logs also enough where it is failing. The host is unhealthy means host service is unhealthy means that is related to that all containers. If anyone container is unhealthy then that is reflecting the source.Host CDC services status help.So we need to check that where it is failing and why for that analyze the logs. That is important. Analyze the logs and also check that what is the traffic rate. CPU is a CPU and the memory spike is high or not.So that also causes the system to be unhealthy sometimes.I think we covered on.Oh.And then no status. Maybe another one.Flow status. You already know that OK. If any particular flow is moving from online or active to some other state, then this alert will be triggered. So we have to check.CDC flow logs, API logs, onprem logs for this one. So CDC flow is initiating that conflict push CDC.So in CDC flow initiating, CDC API generates that status for that particular flow and sends to the status service. If configuration send not applied for the status is dependent on that in the CDC API.The configurations are applied and failing that may be related to the onprem. You can go to that onprem component and check that one why it is failing OK.So that's how the you can validate that why this the flow is a failed OK.The flow status the can easily verify that the from the container at the pod logs.So you you want me to share more details about what type of the different errors. One is that preview details, pending status online. Pending status means configurations are not pushed to the onprem.OK, that may be the reason for this failure also. OK.And what else are there?Service status covered, flow status covered, comprehensive alerts.We have added some more in the Uh alerts.Think the event related. We can check it here.High CPU alate CPU is straightforward. We need to check that which component is causing for this high CPU. Is it a CDC related or some other? If it is a CDC related, which pod is the causing this one?In container, out container, what is the reason? Is it in the topic is increased or maybe memory leak? So we have to check that way.And memory one is also memory and disk which is you can you guys can directly go and check this in the onprem host and host inactive alert. That particular host is not available. Sometimes it may be offline right? You can check that status also.And low EPS the and it is a one of the allied. Then the the traffic is the normal example the 50K QPS. It's continuously sending the data.Due to the deployment or some other reasons, it all of a sudden the traffic is the fall down to 20 KQPS right? Then we need to start analyzing that why the traffic is the reduced? Is it the source level itself? It is the reduced or?And intermittently due to that our components it reduced the poster deployment and we have to start analyzing that one.So that does all that critical things.Thank you. No active flows. This is another one customer enabled ADC service, but there are no active flows configured. Why do we need to keep that in the in that state? We can check with that in the support team.A part of that in down actions.And pending event backlog. There are two metrics. One is that the when we receive the data, we'll count that into the pending event. Once it is processed, we'll subtract from that and increment that in processed. That means pending events and processed events. These are the two metrics.The pending event is keep on increasing and process stays the normal. That means something wrong with that the particular component. It is pending in the in in container itself or maybe out container. Also it is received but not sending to the destination.So these are the things we need to analyze at the onprem level.I hope we covered most of the things. Any queries?
Vaibhav Shetty stopped transcription
